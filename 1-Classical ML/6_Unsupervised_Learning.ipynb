{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_ad_ai_course/blob/main/Classical%20ML/6_Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roZe4RLA8WaU"
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbwlFBil8ooj"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEs6KNnC8fOZ"
   },
   "source": [
    "In this lab, we will use the [Iris](https://scikit-learn.org/stable/datasets/index.html#iris-dataset) open dataset and scikit-learn clustering algorithms to effectively return the species of all those flowers!\n",
    "\n",
    "The objective of this lab is to understand how clustering can help us identify \"potential\" labels of our dataset, which we can later on feed to supervised algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sftvEbc8yVy"
   },
   "source": [
    "As we know, only 0.1% of real-world data has a label of each item (imagine an image and, next to it, a list of all the animals represented). This is what we call *annotated data*. However, annotation is often:\n",
    "\n",
    "- Expensive\n",
    "- Difficult to standardize\n",
    "- Difficult to scale\n",
    "- Sensitive to design changes\n",
    "- Hard to do right\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIVzG82494HY"
   },
   "source": [
    "<img src=\"https://www.dropbox.com/scl/fi/g5lkwhrbsod6xps2b9fus/unlabeled-data-car.jpg?rlkey=rqe4ge1ctchi5z28epalgusdo&raw=1\"  align=\"center\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpqssLLd-C0g"
   },
   "source": [
    "Examples of non-annotated data include:\n",
    "\n",
    "- Chats\n",
    "- Audio\n",
    "- Medical histories\n",
    "\n",
    "So what can we do in that case? One of the common algorithms to attack this problem is _clustering_.\n",
    "\n",
    "Clustering is a set of algorithms that find intrinsic associations in our data to end up with something like this:\n",
    "\n",
    "![Clustering example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/clustering-example.png)\n",
    "\n",
    "There are two fundamental techniques for this: *hierarchical* and *non-hierarchical* clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDnjgea0-JIT"
   },
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUqBVPh5-OWG"
   },
   "source": [
    "In Hierarchical clustering, which is also called agglomerative, we add elements in a nested way. For example:\n",
    "\n",
    "![Hierarchical clustering example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/hierarchiecal-clustering.png)\n",
    "\n",
    "To do this, we do the following:\n",
    "\n",
    "1. Start first cluster with the two elements closest to each other\n",
    "2. Calculate the distance between this cluster and the other elements\n",
    "3. Create a new cluster or append to existing one, based on which have the least distance (we append, by treating the new cluster as another element, if it is closest to the element)\n",
    "4. Continue\n",
    "\n",
    "Easy right?\n",
    "\n",
    "Let's do it in Python! First we will import all the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iHw61vt8ejx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqZIYYpJ-VV6"
   },
   "source": [
    "We will now load the dataset, which is trivial in scikit-learn given its dataset loading abilities, and fit the hierarchical clustering model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVBhqTMi-W78",
    "outputId": "98c8281a-5271-4015-b1f5-8531bf100b80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "model = AgglomerativeClustering(n_clusters=3, compute_distances=True)\n",
    "model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLLivXZY_S1E"
   },
   "source": [
    "It is *THAT* easy! Let's see how well we performed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMKeWgZj_REk",
    "outputId": "3c249f1f-13b6-46d0-d058-38914c81b05a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, the number of clusters predicted by the models are 3\n",
      "Accuracy of hierarchiecal clustering with clusters specified is 89.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f'As expected, the number of clusters predicted by the models are {model.n_clusters_}')\n",
    "\n",
    "predicted_labels = model.labels_\n",
    "corrected_predicted_labels = np.where((predicted_labels==0)|(predicted_labels==1), predicted_labels^1, predicted_labels) # Hierarchiecal starts labels with 1 and the dataset with 0\n",
    "real_labels = iris.target\n",
    "equal = 0\n",
    "for predicted_label, real_label in zip(corrected_predicted_labels, real_labels):\n",
    "    if  predicted_label == real_label:\n",
    "        equal += 1\n",
    "\n",
    "\n",
    "print(f'Accuracy of hierarchiecal clustering with clusters specified is {100*equal / len(corrected_predicted_labels)} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFgHYogX_a8c"
   },
   "source": [
    "Not so bad for a split second operation, right?! What this will do is:\n",
    "\n",
    "1. Get a random flower\n",
    "2. Calculate the distance of it against all other elements\n",
    "3. Create a \"cluster\" of size 2 with the shortest sample\n",
    "4. Iterate until all elements are in a cluster\n",
    "\n",
    "What if we wanted to plot this? This would be as easy as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "BazaMRra_WvK",
    "outputId": "16b7dc4a-55f8-425f-be64-a13863cdcbbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIMMaJ0eAQfW"
   },
   "source": [
    "Of course you may point out that it's easy when you know *ahead of time* that there are three clusters! And you would be right, but this example is just for you to get your feet wet and start learning.\n",
    "\n",
    "There are multiple ways of optimizing the number of clusters hyperparameter, but that is out of scope for this introduction. If you're curious, I recommend checking out [_Hands-On Unsupervised Learning Using Python_](https://learning.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/) by Ankur A. Patel (O'Reilly).\n",
    "\n",
    "For know, lets move on to non-hierarchical clustering and its most prominent example: _k_-means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfMWB6iAAUiY"
   },
   "source": [
    "### Non-Hierarchiechal Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiGuQiXBAb9U"
   },
   "source": [
    "_k_-means is the most famous clustering algorithm of all, and for a good reason:\n",
    "\n",
    "- It is stable\n",
    "- It scales\n",
    "- It is reliable\n",
    "- It is well studied\n",
    "\n",
    "In a nutshell, the objective of _k_-means is to find inherent clusters using the following recipe:\n",
    "\n",
    "1. Start with _k_ randomly chosen landmarks for starting our clusters.\n",
    "1. Assign every element to a cluster based on minimum distance (which can be anything we chose!).\n",
    "1. Now that there are _k_ clusters, optimize them.\n",
    "1. Calculate the centers of each cluster. This is done by averaging them out. These are the new landmarks for the next iteration.\n",
    "1. Continue until there is no reassignment in step 2.\n",
    "\n",
    "This process is represented by the following:\n",
    "\n",
    "![_k_-means Example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/kmeans.png )\n",
    "\n",
    "First to perform _k_-means, we need to perform our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jetXSoHB_hys"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "np.random.seed(42)\n",
    "del predicted_labels\n",
    "del model\n",
    "del predicted_label\n",
    "del corrected_predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtZe2UvTAjDu"
   },
   "source": [
    "We will now load the dataset, which is trivial in scikit-learn with its dataset loading abilities, and then fit the _k_-means model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqivnRzXAg2D",
    "outputId": "33e70261-a5ca-4309-f53e-c90f501e87c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "model = KMeans(n_clusters=3, init='random', max_iter=2000, tol=10**-7, random_state=42, n_init='auto')\n",
    "model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFIss4mSAqr2"
   },
   "source": [
    "Let's compare it for a second with the clustering method we saw before by checking it's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgPr-tCKAkhA",
    "outputId": "783c0fef-8f94-438b-b102-9c3b0e7e42f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of _k_-means clustering with clusters specified is 89.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.labels_\n",
    "where_0 = np.where(predicted_labels == 0)\n",
    "where_1 = np.where(predicted_labels == 1)\n",
    "\n",
    "predicted_labels[where_0] = 1\n",
    "predicted_labels[where_1] = 0\n",
    "real_labels = iris.target\n",
    "equal = 0\n",
    "for predicted_label, real_label in zip(predicted_labels, real_labels):\n",
    "    if  predicted_label == real_label:\n",
    "        equal += 1\n",
    "\n",
    "\n",
    "print(f'Accuracy of _k_-means clustering with clusters specified is {100*equal / len(predicted_labels)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILhF7rOjA2vl"
   },
   "source": [
    "#### Figuring out the number of clusters\n",
    "\n",
    "We can see that it doesn't seem like we have much difference! However, there _is_ a difference, because in _k_-means we have an analytical way of analyzing how to optimize it! Basically, we can calculate for each parameter the inertia of the resulting cluster, so we can optimize over it. This way, we can get the optimum number of clusters *without* knowing them ahead of time!\n",
    "\n",
    "Let's try to do the same as before, but with an increasing number of clusters, and then compare the inertias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8wIn92TAuqs",
    "outputId": "0571cca0-60f0-4a81-b8d9-addc9172aa35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertias were: [681.3706, 152.3479517603579, 78.851441426146, 57.22847321428572, 46.446182051282065, 39.03998724608726, 34.46949589883801, 30.1865551948052]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = [1,2,3,4,5,6,7,8]\n",
    "inertias = []\n",
    "for cluster in n_clusters:\n",
    "    del model  # So we remove previous references\n",
    "    model = KMeans(n_clusters=cluster, random_state=42, n_init=10)\n",
    "    model.fit(X)\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "print(f'The inertias were: {inertias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-MK5wbQBcPS"
   },
   "source": [
    "If you check closely, the inertias decrease over time. There is a point, however, where the decrease isn't as steep. This means that we aren't gaining too much information by adding more clusters.\n",
    "\n",
    "The *Elbow method* dictates that the point where the decrease in inertia starts to be \"low\" is the optimal number of clusters. So for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgTkx8dpBc2v",
    "outputId": "73ed9885-d916-48cd-d711-6fbf21fecde0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The differences in each step are: [529.0226482396421, 73.49651033421189, 21.622968211860275, 10.782291163003656, 7.406194805194808, 4.570491347249245, 4.282940704032811]\n"
     ]
    }
   ],
   "source": [
    "print(f'The differences in each step are: {[(i-j) for i, j in zip(inertias[:-1], inertias[1:])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYqn0GB8BkY3"
   },
   "source": [
    "We can see that going from one cluster to two reduced the inertia by 77%, then we reduced it by another 48% but later on it only reduced by a bit (20 or less); so, we could say the optimal number of clusters is three. What we do then is a hierarchical clustering and confirm this in a dendrogram, as we did before.\n",
    "\n",
    "However, _k_-means is not always the best algorithm since it expects clusters to be globular and of equal shape. Let's quickly analyze this in the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIyy7GDEBpdc"
   },
   "source": [
    "### DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4UxF_leBr8G"
   },
   "source": [
    "If you find that _k_-means is not working for you, it might be because your data is not equivariant or globular; if so, there are other alternatives! One famous algorithm to revisit is *DB Scan*.\n",
    "\n",
    "The idea behind DB Scan is to not form clusters by assigning all members and later on reduce a metric (inertia in _k_-means), but to check a cluster by the *density of its members*, characterized by the minimum number of elements needed to perform a cluster and the radius of the search space.\n",
    "\n",
    "This way, DB Scan \"scans\" the whole map looking for neighbors and assigns in case there are sufficient, and sufficiently close! Check out this [video][dbscan] for an example.\n",
    "\n",
    "[dbscan]:  http://primo.ai/index.php?title=Density-Based_Spatial_Clustering_of_Applications_with_Noise_(DBSCAN) \"DB Scan Example\"\n",
    "\n",
    "So to get a handle on this in code, first we do our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8OyN4bHBfhQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl2p7fcIBy-O"
   },
   "source": [
    "Later on we might create artificial data, but for this we use the super useful `make_blobs` method from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJEGOzd7BxX5"
   },
   "outputs": [],
   "source": [
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80_cC2A1B2DP"
   },
   "source": [
    "Next we'll perform the scan, setting the ratio as 0.3 and the minimum elements to create a cluster as 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NR8GqOXB0ft"
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "labels = db.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wVCZsD8B7hJ"
   },
   "source": [
    "Finally, let's see how it goes! Run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BcwUqytB5-O",
    "outputId": "f7450761-4789-418c-fb96-3d5d5a92752a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 18\n",
      "Homogeneity: 0.953\n",
      "Completeness: 0.883\n",
      "V-measure: 0.917\n",
      "Adjusted Rand Index: 0.952\n",
      "Adjusted Mutual Information: 0.916\n",
      "Silhouette Coefficient: 0.626\n"
     ]
    }
   ],
   "source": [
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHN84zZLCB5M"
   },
   "source": [
    "Not too shabby! DB Scan automatically discovered the three clusters and also removed the 18 outliers, setting them as not being in any cluster!\n",
    "\n",
    "DB Scan is a bit slower than _k_-means and does not parallelize as the latter does, but it is extremely good at finding the \"correct clusters.\" Using the three tools we have seen, we have a perfect toolkit to find the correct number of tentative labels for our data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2pnyqh2CI3P"
   },
   "source": [
    "To recap, we did the following:\n",
    "\n",
    "- Learned about unsupervised learning and the types of clustering\n",
    "- Performed a hierarchical clustering, finding out how to visually represent our clusters\n",
    "- Migrated into non hierarchical clustering checking out the most famous algorithm of all, _k_-means, and look at its assumptions and how to find out the number of clusters to set\n",
    "- Finally, we briefly explored DB Scan to analyze density based clustering and how it helps us find, intrinsically, any outliers and the correct number of clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHsXCm1qB-Z4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcwgbaBICLFD"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udaCVvaNCtsJ"
   },
   "source": [
    "In this section we will use the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) open dataset and scikit-learn to effectively classify the correct famous person from our images!\n",
    "\n",
    "The objective of this section is to understand how PCA can help us identify representative new curated features that allow for easier calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Diu14dRC8BE"
   },
   "source": [
    "In the previous section, we learned how we can find aggregations of our non-annotated dataset with clustering. However, as the number of dimensions increases drastically, that operation becomes more complex and less robust, since in a highly dimensional space, outliers are more difficult to find! A collection of images presents a good example of the number of dimensions we may need to treat.\n",
    "\n",
    "A normal, super low definition color image has the following axes: width x height x length x color scale. This means that a simple RGB image has 256 x 256 x 256 x 3, or _50 million_ different values! That is incredibly large, and we aren't even talking about 4k videos or audio!\n",
    " <!-- (which have even more values, since this is only one frame and a low definition video has 30 of those PER second) -->\n",
    "\n",
    "As a result, you may guess that we need to reduce this dimensionality into key numbers that contain \"most\" of the information: and now we are in the domain of *dimensionality reduction*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGbX2oHlC8Ik"
   },
   "source": [
    "The basic idea of dimensionality reduction can be described as:\n",
    "\n",
    "- Given a dataset with n variables, probably correlated\n",
    "- Find a new new dataset with _p_ variables that explain the same\n",
    "- Have a way of going back!\n",
    "\n",
    "Here is an illustration of this idea:\n",
    "\n",
    "![PCA example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/pca.png)\n",
    "\n",
    "Why is this useful? It's useful for many, many reasons:\n",
    "\n",
    "- We can compress data to be able to send it easily\n",
    "- We can use this technique before any other supervised learning in order to make our computations faster\n",
    "- It's cool!\n",
    "\n",
    "Let's see how to do this in Python, first with a simple Iris lab. As always, let's first import the necessary modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrfVcloQCm28"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUgmBcQXDIba"
   },
   "source": [
    "Later on, we define the dataset and we perform the PCA algorithm, specifying the number of components we want to retain (i.e., the number of dimensions we want to keep):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "Hjdq9oSVDHOb",
    "outputId": "519b0f52-f762-446e-f68a-9c19b03f9e83"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMLLR75FDPDE"
   },
   "source": [
    "That's it, we did it! In one line, we did that super complex operation. Incredible, right?!\n",
    "\n",
    "Up to this point, we *trained* a PCA algorithm on the directions we want to keep, and now we will *project* our values (which can have many dimensions) into the one that the algorithm found!\n",
    "\n",
    "You can see an example in the following, where projecting the original values into the first two components (PC1, PC2) retained most of the information:\n",
    "\n",
    "![PCA example 2](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/pca2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-r8Bdv8TDKUY",
    "outputId": "3255afff-04b5-48d7-cd7b-8c404ce4c949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of old values are (150, 4)\n",
      "Shape of new transformed values are (150, 1)\n"
     ]
    }
   ],
   "source": [
    "new_X = pca.transform(X)\n",
    "print(f'Shape of old values are {np.shape(X)}')\n",
    "print(f'Shape of new transformed values are {np.shape(new_X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERkRSsSeDVRS"
   },
   "source": [
    "As we can imagine, now the dimension of the new `X` is `number_of_samples` times 1, because we retained one component!\n",
    "\n",
    "You might be wondering: how much of the information we retained? For that, we have the percent of explained variance, which measures how much we \"lost\" in transforming our original data to this new dataset (the mathematics about this are really complex, so just trust me here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaCHvPhTDR90",
    "outputId": "d33ac2d8-1c54-4136-8675-c6252ea3a814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We only lost 7.538127679827284% of information\n"
     ]
    }
   ],
   "source": [
    "print(f'We only lost {100-100*sum(pca.explained_variance_ratio_)}% of information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-xQP8LQDbZX"
   },
   "source": [
    "Pretty incredible, right?\n",
    "\n",
    "And now we can do whatever we want! For example, we can perform _k_-means and check if we can do better than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4vk31C2DYVB",
    "outputId": "e6ec10c3-a5f6-440c-cf8a-6cb19b343b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 2 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 2 2\n",
      " 2 0]\n",
      "Accuracy of K Means clustering with clusters specified is 91.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3, init='random', max_iter=2000, tol=10**-7, random_state=42, n_init=10)\n",
    "model.fit_predict(new_X)\n",
    "predicted_labels = model.labels_\n",
    "print(predicted_labels)\n",
    "# where_2 = np.where(predicted_labels == 2)\n",
    "where_1 = np.where(predicted_labels == 1)\n",
    "where_0 = np.where(predicted_labels == 0)\n",
    "\n",
    "# predicted_labels[where_2] = 1\n",
    "predicted_labels[where_0] = 1\n",
    "predicted_labels[where_1] = 0\n",
    "real_labels = iris.target\n",
    "equal = 0\n",
    "for predicted_label, real_label in zip(predicted_labels, real_labels):\n",
    "    if  predicted_label == real_label:\n",
    "        equal += 1\n",
    "\n",
    "print(f'Accuracy of K Means clustering with clusters specified is {100*equal / len(predicted_labels)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07SJ3gfvDuj1"
   },
   "source": [
    "By squashing into a single component, the whole clustering just got WAY easier! And of course, we can revert and go back, so it is _that_ incredible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAGVsxwhDxcV"
   },
   "source": [
    "### Playing with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELm_Yo6dDz6j"
   },
   "source": [
    "Now that we know about PCA and how it leverages the reduction of dimensionality, let's look at best way to use this knowledge that play around with images!\n",
    "Images are a perfect case for PCA, since we can reduce those terrible 50 million points to just a few.\n",
    "\n",
    "We will create what is known as a *Machine Learning Pipeline* that will use PCA to make images more tractable, which will then help us pass them through a classifier to predict faces. Note that it is *not* the objective of this lab to explain all the parts of the classifier, so don't worry about that!\n",
    "\n",
    "This example comes from the excellent [scikit-learn docs](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py), and I highly recommend taking a look at them.\n",
    "\n",
    "First, as always, let's import everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtFHexUDhuY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "np.random.seed(42)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyugRp1wD4Ao"
   },
   "source": [
    "Next, we will load the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SI45eZ7zD2MW",
    "outputId": "6f7a70f5-42d2-4925-b8d7-81954d118187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clBbDtzrEB3f"
   },
   "source": [
    "Note that we have around 1,200 images with 1,850 dimensions (so images will be a bit blurry), and 7 different people!\n",
    "\n",
    "What we will do now is split our dataset between training, test, and validation sets so we can check the performance of our pipeline later on. This is a normal practice in Supervised Learning (since the labels already exist!), because we may want to compare multiple classifiers.\n",
    "\n",
    "As that can take a bunch of time, we will first reduce the dimension to 150 components with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M8-D9NvD6Pv",
    "outputId": "d45f225c-f70c-4af2-c028-223ebeae425e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 150 eigenfaces from 966 faces\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWOr1ebgEJXd"
   },
   "source": [
    "A key factor is that we trained our PCA with the training set, and later on transformed the other sets with *that exact trained PCA*; we did not train another one! Let's try to debug how much of the variance remained by keeping only 150 components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpMHqsijEGcF",
    "outputId": "90770bc1-ac27-4bae-a29f-be9c53147e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We only lost 5.330208141822368% of information\n"
     ]
    }
   ],
   "source": [
    "print(f'We only lost {100-100*sum(pca.explained_variance_ratio_)}% of information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5dy6J5lELcp"
   },
   "source": [
    "Finally, we train an SVM classifier with multiple possible hyperparameters using cross validation (a common supervised learning technique) to get the final classifier and check its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bv3AzfQPEK2H",
    "outputId": "543e6073-63cf-460e-9120-493edd04bb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.88      0.54      0.67        13\n",
      "     Colin Powell       0.80      0.88      0.84        60\n",
      "  Donald Rumsfeld       0.94      0.63      0.76        27\n",
      "    George W Bush       0.85      0.98      0.91       146\n",
      "Gerhard Schroeder       0.95      0.80      0.87        25\n",
      "      Hugo Chavez       1.00      0.53      0.70        15\n",
      "       Tony Blair       0.91      0.83      0.87        36\n",
      "\n",
      "         accuracy                           0.86       322\n",
      "        macro avg       0.91      0.74      0.80       322\n",
      "     weighted avg       0.87      0.86      0.86       322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel='rbf', class_weight='balanced'), param_grid\n",
    ")\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)  # Check that we use the transofrmed test set on the PCA trained on the TRAIN set!\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K8TgXowESxv"
   },
   "source": [
    "We can verify that we have really high accuracy (>85%), considering we just took two seconds to run this! The magic lies in the PCA algorithms that allowed us to reduce much of the \"clutter\" into valuable information. I encourage you to try it out at home with greater or fewer components to see how it differs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-ev5ucGEY7M"
   },
   "source": [
    "To recap, we did the following:\n",
    "\n",
    "- Learned about dimensionality reduction\n",
    "- Performed PCA on the Iris dataset and verified that with less dimensions we can easily distinguish information like the clusters\n",
    "- Performed a _Machine Learning Pipeline_ by first compressing images in order to set an SVM classifier later on; we retained a lot of the information and quickly got to an >85% classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr26iDcsEj7g"
   },
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n0DX8h2Encg"
   },
   "source": [
    "In this section, we will explore both outlier and anomaly detection algorithms to improve our estimates.\n",
    "\n",
    "The objective of this section is to understand how outliers can be detected in several ways. By removing them from the training set, we can obtain better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv7ceuZMEsX0"
   },
   "source": [
    "Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (i.e., an _inlier_), or should be considered as different (i.e., an _outlier_). However, we must distinguish between:\n",
    "\n",
    "- Outlier detection: when we try to find spurious data in our dataset\n",
    "- Novelty detection: when we try to determine if a new datapoint belongs to the same distribution as ours\n",
    "\n",
    "The main differentiator is that in the former case, we can assume that outliers are in low density regions (since they are \"odd\"), however in the latter, we may have a new \"cluster\" of new data points, which makes the problem way harder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBSbBvB-EvR8"
   },
   "source": [
    "### Outlier Detection\n",
    "\n",
    "One of the most common outlier detection algorithms is Local Outlier Factor (LOF), which basically calculates the \"amount of neighbors per unit of distance\" to determine a density per datapoint, and then, with a given threshold, it determines whether each datapoint is an inlier or outlier. It is quite powerful for such a simple idea!\n",
    "\n",
    "Let's take a look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByRSq94ZExgr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a0luWv4E1As"
   },
   "source": [
    "Now we will generate some data with outliers to verify how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geYpY1pSEzpy"
   },
   "outputs": [],
   "source": [
    "X_inliers = 0.3 * np.random.randn(100, 2)\n",
    "X_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.r_[X_inliers, X_outliers]\n",
    "n_outliers = len(X_outliers)\n",
    "ground_truth = np.ones(len(X), dtype=int)\n",
    "ground_truth[-n_outliers:] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyKkdcy3E4OZ"
   },
   "source": [
    "We then train our model to classify correctly the outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGOsk9RaE2sE",
    "outputId": "48a6b0d5-6978-43ed-c4f0-f36e592bfc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on finding outliers is 96.36363636363636 %\n"
     ]
    }
   ],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = clf.fit_predict(X)\n",
    "print(f'The accuracy on finding outliers is {100*(y_pred == ground_truth).sum()/len(X)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws6TwE5LE8XU"
   },
   "source": [
    "It's pretty amazing how easy it is! For example, if we get the final datapoint, let's calculate its score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmHg2hKEE6lQ",
    "outputId": "9b459aaf-09a3-4000-bec5-8a404a2e9f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in density score between the last datapoint (which was outlier) and the first datapoint (which was inlier) is 384.58157325255985 %\n"
     ]
    }
   ],
   "source": [
    "print(f'The difference in density score between the last datapoint (which was outlier) and the first datapoint (which was inlier) is {-100*(clf.negative_outlier_factor_[0] - clf.negative_outlier_factor_[-1])/clf.negative_outlier_factor_[0]} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgvcvhGPFBhk"
   },
   "source": [
    "Of course, there are multiple other algorithms for outlier finding. I recommend reading the [scikit-learn docs](https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection). But for now, let's switch to another efficient algorithm is good to have in mind: Isolation Forests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8rT_I1TFFDB"
   },
   "source": [
    "### Isolation Forests\n",
    "\n",
    "Now that we know how to find outliers with LOF, let's discover another method that is highly stable in high dimensions: Isolation Forests.\n",
    "\n",
    "The idea is quite complex but the implementation is dead easy!\n",
    "\n",
    "The idea is that if we select a feature by random, and create a decision tree over it then we would split our data. If we do this recursively then we have a decision tree to split our data points; however it is seen that outliers have shorter paths to be split from the rest, since they are spatially separated; ergo we can calculate the average path over all possible decision trees and the ones with lower scores are outliers.\n",
    "\n",
    "In code it looks like this, where 100 is obviously an outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzrj_dYAE-Ql",
    "outputId": "f20d6ffc-178e-4b67-85a0-090f0dc6e7ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "X = [[-1.1], [0.3], [0.5], [100]]\n",
    "clf = IsolationForest(random_state=42).fit(X)\n",
    "clf.predict([[0.1], [0], [90]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZjH68bNFMoA"
   },
   "source": [
    "We can verify that the forest correctly assigned a -1 to 90, marking it as an outlier. Let's see how it compares with LOF in the example from before! As always, we first import everything and create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7lbP8AqFK9X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "np.random.seed(42)\n",
    "\n",
    "X_inliers = 0.3 * np.random.randn(100, 2)\n",
    "X_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.r_[X_inliers, X_outliers]\n",
    "n_outliers = len(X_outliers)\n",
    "ground_truth = np.ones(len(X), dtype=int)\n",
    "ground_truth[-n_outliers:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxA9PBHwFPFY",
    "outputId": "048c6afd-62af-4eb1-84e4-1f09307cd181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on finding outliers is 96.36363636363636 %\n"
     ]
    }
   ],
   "source": [
    "clf = IsolationForest(random_state=42).fit(X)\n",
    "y_pred = clf.predict(X)\n",
    "print(f'The accuracy on finding outliers is {100*(y_pred == ground_truth).sum()/len(X)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud05YfTlFUdS"
   },
   "source": [
    "Again we got a remarkably high accuracy! Next, let's compare both methods in a more complex dataset.\n",
    "\n",
    "### Comparing LOF and Isolation Forests\n",
    "\n",
    "First let's import everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKDXo7XlFRrL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "np.random.seed(42)\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE4eGNnDFaVA"
   },
   "source": [
    "Now we create the datasets. We will create a unimodal, bimodal, bimodal with different variance, and a moon dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZqnyIqsFYQx"
   },
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "outliers_fraction = 0.15\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "n_inliers = n_samples - n_outliers\n",
    "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "datasets = {'unimodal':\n",
    "    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,\n",
    "               **blobs_params)[0],\n",
    "    'bimodal': make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],\n",
    "               **blobs_params)[0],\n",
    "    'bimodal and not equivariant': make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],\n",
    "               **blobs_params)[0],\n",
    "    'moon shaped': 4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -\n",
    "          np.array([0.5, 0.25]))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eej_WSV6Fevo"
   },
   "source": [
    "Now we will compare the efficiency of both algorithms in all settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AkoBywFFdDg",
    "outputId": "557735b6-46fe-4f4c-b5a7-6bd91107409c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on finding outliers with LOF and dataset unimodal is 100.0 %\n",
      "The accuracy on finding outliers with Isolation Forests and dataset unimodal is 100.0 %\n",
      "The accuracy on finding outliers with LOF and dataset bimodal is 99.33 %\n",
      "The accuracy on finding outliers with Isolation Forests and dataset bimodal is 99.33 %\n",
      "The accuracy on finding outliers with LOF and dataset bimodal and not equivariant is 89.33 %\n",
      "The accuracy on finding outliers with Isolation Forests and dataset bimodal and not equivariant is 90.67 %\n",
      "The accuracy on finding outliers with LOF and dataset moon shaped is 85.8 %\n",
      "The accuracy on finding outliers with Isolation Forests and dataset moon shaped is 87.54 %\n",
      "Comparison done!\n"
     ]
    }
   ],
   "source": [
    "for name, X in datasets.items():\n",
    "    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n",
    "    ground_truth = np.ones(len(X), dtype=int)\n",
    "    ground_truth[-n_outliers:] = -1\n",
    "    isolation = IsolationForest(contamination=outliers_fraction, random_state=42)\n",
    "    isolation.fit(X)\n",
    "    lof = LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction)\n",
    "    lof.fit(X)\n",
    "    y_pred_lof = lof.fit_predict(X)\n",
    "    y_pred_isol = isolation.fit(X).predict(X)\n",
    "    print(f'The accuracy on finding outliers with LOF and dataset {name} is {round(100*(y_pred_lof == ground_truth).sum()/len(X),2)} %')\n",
    "    print(f'The accuracy on finding outliers with Isolation Forests and dataset {name} is {round(100*(y_pred_isol == ground_truth).sum()/len(X),2)} %')\n",
    "\n",
    "print('Comparison done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHZOwUGwFmhb"
   },
   "source": [
    "We can verify that Isolation Forests was slightly better in \"weirder data\" and LOF was better in data coming from a known distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0AvaWu0FzcO"
   },
   "source": [
    "### Novelty Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4UNblPkF1eN"
   },
   "source": [
    "As a closing section, let's try to do novelty detection! The harder part in this topic is that there may be clusters of new data that correspond to a novel distribution, but our old algorithms wouldn't detect them.\n",
    "\n",
    "In more formal terms, the idea is to learn a rough, close frontier delimiting the contour of the initial observations distribution plotted in embedding *p*-dimensional space. Then, if further observations are within the frontier-delimited subspace, they are considered as coming from the same population as the initial observations. Otherwise, if they are outside the frontier, we can say that they are abnormal with a given confidence in our assessment.\n",
    "\n",
    "We can use LOF for novelty detection, since we may estimate that a new datapoint is probably anomalous if it is far from all the other clusters. It is not the most famous anomaly detection algorithm, but it is a nice start since we already know how to play with it!\n",
    "\n",
    "First we do our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC_Hm1E8FghK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import svm\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ7p2PlFF3cN"
   },
   "outputs": [],
   "source": [
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "X_novel = np.random.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv4XVRhPF8D8"
   },
   "source": [
    "Now we train our LOF with the training data, and then verify with the test data (not novel) and the novel dataset (novel) and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6amsjPbXF6KR",
    "outputId": "903a21b8-547d-42e6-965f-1841aa5a827a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on non-novel distributions: 80.0 \n",
      "Accuracy on novel distributions: 95.0 \n"
     ]
    }
   ],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_novel = clf.predict(X_outliers)\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_novel = y_pred_novel[y_pred_novel == 1].size\n",
    "\n",
    "print(f'Accuracy on non-novel distributions: {round(100*(1- (n_error_test/len(X_test))),2)} ')\n",
    "print(f'Accuracy on novel distributions: {round(100*(1- (n_error_novel/len(X_novel))),2)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtWnB-_xGG16"
   },
   "source": [
    "Not so shabby for a simple outlier detection used as novelty!\n",
    "\n",
    "Let's now learn about another algorithm that was designed for this task: one class SVM with RBF kernels. This is best described in the original paper: Schlkopf, Bernhard, et al., \"Estimating the support of a high-dimensional distribution,\" *Neural Computation* 13.7 (2001): 1443-1471.\n",
    "\n",
    "One class SVM with kernel learns a decision function, like all SVM algorithms do, to maximize the margin of our training data in a robust way. When we mix this with non-linear kernels, we end up with really interesting decision boundaries to determine if new data is novel or not! Here's an illustration:\n",
    "\n",
    "![SVM example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/svm.png)\n",
    "\n",
    "On the same dataset as before, we now run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXwdSkjcF9ze",
    "outputId": "45d0c7a1-8d4a-4d99-bc02-ecf006564cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 89.5 \n",
      "Accuracy on non-novel distributions: 82.5 \n",
      "Accuracy on novel distributions: 100.0 \n"
     ]
    }
   ],
   "source": [
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_novel = clf.predict(X_novel)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_novel = y_pred_novel[y_pred_novel == 1].size\n",
    "\n",
    "print(f'Accuracy on training data: {round(100*(1- (n_error_train/len(X_train))),2)} ')\n",
    "print(f'Accuracy on non-novel distributions: {round(100*(1- (n_error_test/len(X_test))),2)} ')\n",
    "print(f'Accuracy on novel distributions: {round(100*(1- (n_error_novel/len(X_novel))),2)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPVgTbcgGPVf"
   },
   "source": [
    "As we can check by comparison, SVM with RBF kernel is pretty good at detecting novel data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5TrXVsvGRyq"
   },
   "source": [
    "To recap, we did the following:\n",
    "\n",
    "- Learned about outlier and novelty detection\n",
    "- Performed outlier detection both with LOF as well as Isolation Forests, comparing the optimality of each method in each case\n",
    "- Learned about novelty detection with both LOF and SVM with RBF kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjZFi840Go0A"
   },
   "source": [
    "# Now you do it\n",
    "<img src=\"https://www.dropbox.com/scl/fi/76r2mea69m6mdinnaqweh/hands_on.jpg?rlkey=bchhotvwfjzgb7hc35igxxphs&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNdbGXQ4GquU"
   },
   "source": [
    "We will work with the [breast cancer proteomes](https://www.dropbox.com/scl/fi/1lmsdumz7wbpi041o2wq6/77_cancer_proteomes_CPTAC_itraq.csv?rlkey=6xl7ozj88z2iwv607xhxc70kx&dl=0) and [clinical data](https://www.dropbox.com/scl/fi/0w1ir24w99b06azh3uspu/clinical_data_breast_cancer.csv?rlkey=j71xv3qbdickar92pfjff3om2&dl=0)\n",
    "The objective of this exercise is to understand how unsupervised learning can provide insights to later merge in the pipeline before finalizing with supervised techniques to solve a machine learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9Q9T9p_IIB4",
    "outputId": "d2209dac-46e9-449a-e199-c726e9118927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f ./77_cancer_proteomes_CPTAC_itraq.csv ]; then\n",
    "  wget -O ./77_cancer_proteomes_CPTAC_itraq.csv https://www.dropbox.com/scl/fi/1lmsdumz7wbpi041o2wq6/77_cancer_proteomes_CPTAC_itraq.csv?rlkey=6xl7ozj88z2iwv607xhxc70kx&dl=0\n",
    "fi\n",
    "if [ ! -f ./clinical_data_breast_cancer.csv ]; then\n",
    "  wget -O ./clinical_data_breast_cancer.csv https://www.dropbox.com/scl/fi/0w1ir24w99b06azh3uspu/clinical_data_breast_cancer.csv?rlkey=j71xv3qbdickar92pfjff3om2&dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcdJvCyXIcS2"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V63EAvN-GMsm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import decomposition\n",
    "import re\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "split_name=re.compile('[.-]')\n",
    "\n",
    "def rename_patients(name):\n",
    "    components=split_name.split(name)\n",
    "    return '-'.join(('TCGA',components[0],components[1]))\n",
    "\n",
    "protein_activity=pd.read_csv(\"./77_cancer_proteomes_CPTAC_itraq.csv\")\n",
    "protein_activity.set_index(['RefSeq_accession_number','gene_symbol','gene_name'],inplace=True)\n",
    "protein_activity.fillna(protein_activity.median(),inplace=True)\n",
    "protein_activity.rename(columns=rename_patients,inplace=True)\n",
    "clinical_data=pd.read_csv(\"./clinical_data_breast_cancer.csv\").set_index('Complete TCGA ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4tkTa0mH870"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJimfKt4anZrS3UXYxYSvp",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
