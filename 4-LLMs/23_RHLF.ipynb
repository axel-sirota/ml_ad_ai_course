{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyOefvxl59PsEsqRjnoC08kJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hxDisDOWywSx"},"outputs":[],"source":["pip install gym tensorflow transformers datasets stable_baselines3 'shimmy>=0.2.1'"]},{"cell_type":"code","source":["import gym\n","from gym import spaces\n","import numpy as np\n","import tensorflow as tf\n","from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n","from datasets import load_dataset\n","import random\n","import tensorflow_hub as hub\n","from stable_baselines3 import PPO\n","import time\n","\n","class SummaryEnv(gym.Env):\n","    \"\"\"\n","    Custom Environment for training FLAN-T5 using PPO with GPT-2 as a reward model.\n","    \"\"\"\n","    def __init__(self, dataset, model_name='t5-small', reward_model_name='google/flan-t5-base', max_length=512):\n","        super(SummaryEnv, self).__init__()\n","\n","        # Load tokenizer and model for FLAN-T5\n","        self.tokenizer_t5 = AutoTokenizer.from_pretrained(model_name)\n","        self.model_t5 = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","        self.reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n","        self.reward_model = TFAutoModelForSeq2SeqLM.from_pretrained(reward_model_name)\n","\n","        # Load dataset\n","        self.dataset = dataset\n","\n","        # Define action and observation space\n","        self.action_space = spaces.Discrete(max_length)  # Adjust as needed\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n","        self.embedding_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","    def step(self, action):\n","        step_start = time.time()\n","        print(\"Step started\")\n","\n","        self.current_input_text = self._action_to_text(action)\n","        action_to_text_time = time.time() - step_start\n","        print(\"Action to text conversion done: \", action_to_text_time)\n","\n","        self.current_summary = self._generate_summary(self.current_input_text)\n","        current_summary_time = time.time() - step_start\n","        print(\"Summary generation done: \", current_summary_time - action_to_text_time)\n","\n","        self.current_ideal_summary = self._generate_ideal_summary(self.current_input_text)\n","        ideal_summary_generation_time = time.time() - step_start\n","        print(\"Ideal summary generation done: \", ideal_summary_generation_time - current_summary_time)\n","\n","        reward = self._evaluate_summary(self.current_summary, self.current_ideal_summary)\n","        reward_evaluation_time = time.time() - step_start\n","        print(\"Reward evaluation done: \", reward_evaluation_time - ideal_summary_generation_time)\n","\n","        done = True\n","        info = {}\n","        observation = np.array([0.0])\n","        print(\"Step completed: \", time.time() -  step_start)\n","        return observation, reward, done, info\n","\n","\n","    def reset(self):\n","        return np.array([0.0])\n","\n","    def render(self, mode='human'):\n","        if mode == 'human':\n","            print(f\"\\n\\n---------------------------------------------\\n\")\n","            print(f\"\\nInput Text: {self.current_input_text}\\n\")\n","            print(f\"\\nGenerated Summary: {self.current_summary}\\n\")\n","            print(f\"\\nIdeal Summary: {self.current_ideal_summary}\\n\")\n","\n","    def _action_to_text(self, action):\n","        return random.choice(self.dataset)\n","\n","    def _generate_summary(self, input_text):\n","        inputs = self.tokenizer_t5.encode_plus(\n","            \"summarize: \" + input_text,\n","            return_tensors=\"tf\",\n","            max_length=512,\n","            truncation=True,\n","            padding='max_length'\n","        )\n","        outputs = self.model_t5.generate(\n","            inputs['input_ids'],\n","            attention_mask=inputs['attention_mask'],\n","            max_length=150,\n","            min_length=40,\n","            length_penalty=2.0,\n","            num_beams=4,\n","            early_stopping=True\n","        )\n","        summary = self.tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n","        return summary\n","\n","    def _generate_ideal_summary(self, input_text):\n","        # Generate summary using T5\n","        inputs = self.reward_tokenizer(input_text, return_tensors='tf', truncation=True, padding=True, max_length=1024)\n","        summary_ids = self.reward_model.generate(inputs['input_ids'], max_length=150, min_length = 40, length_penalty=2.0, num_beams=4, early_stopping=True)\n","        ideal_summary = self.reward_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","        return ideal_summary\n","\n","    def _evaluate_summary(self, summary, ideal_summary):\n","        # Get embeddings of the summaries\n","        summary_embedding = self.embedding_model([summary])\n","        ideal_summary_embedding = self.embedding_model([ideal_summary])\n","\n","        # Compute cosine similarity\n","        cosine_similarity = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(summary_embedding, axis=1),\n","                                                      tf.nn.l2_normalize(ideal_summary_embedding, axis=1)))\n","        return cosine_similarity.numpy()\n"],"metadata":{"id":"rlKKUy9dyxQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load the dataset from Hugging Face\n","dataset = load_dataset(\"knkarthick/dialogsum\", split='validation')\n","dialogues = [item['dialogue'] for item in dataset]\n"],"metadata":{"id":"h_oCCEsfy06I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Create the environment\n","env = SummaryEnv(dataset=dialogues)\n"],"metadata":{"id":"If4yVhyay1-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = PPO(\"MlpPolicy\", env, verbose=2, n_steps=10)"],"metadata":{"id":"jyuCoUEtnCtt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.learn(total_timesteps=5)"],"metadata":{"id":"zOwUfzFPE7rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","obs = env.reset()\n","for _ in range(1):  # Adjust the number of steps\n","    action, _states = model.predict(obs, deterministic=True)\n","    obs, reward, done, info = env.step(action)\n","    env.render()\n","    if done:\n","      obs = env.reset()\n"],"metadata":{"id":"P-kfIZNNy2yL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env._generate_summary(\"\"\"\n","#Person1#: Hello, how are you?\n","#Person2#: I'm fine, thank you. How are you?\n","#Person1#: I'm good, thanks. How can I help you?\n","#Person2#: I'm looking for a restaurant in the area.\n","#Person1#: Sure, what kind of food are you looking for?\n","#Person2#: I'm looking for a restaurant that serves Italian food.\n","#Person1#: What kind of price range are you looking for?\n","#Person2#: Not too expensive.\n","#Person1#: What kind of atmosphere are you looking for?\n","#Person2#: I'm looking for a restaurant that has a romantic atmosphere.\n","#Person1#: I know just the place for you! It's right around the corner, it's called The Italian Bistro.\n","#Person2#: Thank you!\n","\"\"\")"],"metadata":{"id":"1zp6cZpuFeYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TG5xfzAhzAlA"},"execution_count":null,"outputs":[]}]}