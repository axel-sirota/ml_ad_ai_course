{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyNmf9CMTDo3u825O3P5AMLZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Simplified LoRA adaptation of FFN"],"metadata":{"id":"LHd2B-47wlEn"}},{"cell_type":"markdown","source":["We will show how to do LoRA on a simple FFN by first pre-training it on Fashion MNIST and then finetune it on MNIST. As those datasets don't have a ton to do the performance will be quite bad, but we seek to show how to do PEFT in general regardless of the model"],"metadata":{"id":"pqoxVXkCwrRu"}},{"cell_type":"markdown","source":["## Pre-Training"],"metadata":{"id":"lXVXbviLxE6H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er9WwABqnXZo"},"outputs":[],"source":["pip install datasets"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# Load Fashion MNIST dataset\n","(train_images, train_labels), (test_images, test_labels) = None # Load Fashion MNist dataset\n","\n","# Normalize the images\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Define the more complex model\n","model = keras.Sequential([\n","    Flatten(input_shape=(28, 28)),\n","\n","    Dense(1024, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(512, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(256, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(128, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(64, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.3),\n","\n","    Dense(10, activation='softmax')  # 10 classes in Fashion MNIST\n","])\n","\n","# Compile the model\n","None # Compile the model\n","\n","\n"],"metadata":{"id":"VR_yyLKGntCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"9yHTYWEtoRCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","None # Train the model for at least 15 epochs"],"metadata":{"id":"IMWnJHcQoSK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lora-Adaptation"],"metadata":{"id":"yYJjIaefwkJG"}},{"cell_type":"markdown","source":["Load the new dataset"],"metadata":{"id":"Oix8xfCYoEIa"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","\n","# Load MNIST dataset\n","(train_images, train_labels), (test_images, test_labels) = None # Load MNIST dataset\n","\n","# Normalize the images\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Reshape images for the model\n","train_images = train_images.reshape((-1, 28, 28, 1))\n","test_images = test_images.reshape((-1, 28, 28, 1))\n"],"metadata":{"id":"k9LOAy-fn_33"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's implement a LoRA layer, remember the LoRA implementation consists of two low rank dense layers:\n","\n","\n","\n","<img src='https://www.dropbox.com/scl/fi/dfhuc42h5ohcbfny14gg8/lora.png?rlkey=7ku1ocyzibdgmnkup7kmsd8gb&raw=1'  />\n"],"metadata":{"id":"zZAAcOEs9RE_"}},{"cell_type":"code","source":["class LoraLayer(keras.layers.Layer):\n","    def __init__(\n","        self,\n","        original_layer,\n","        rank=8,\n","        num_heads =1,\n","        dim = 1,\n","        trainable=False,\n","        **kwargs,\n","    ):\n","        # We want to keep the name of this layer the same as the original\n","        # dense layer.\n","        original_layer_config = original_layer.get_config()\n","        name = original_layer_config[\"name\"]\n","\n","        kwargs.pop(\"name\", None)\n","\n","        super().__init__(name=name, trainable=trainable, **kwargs)\n","\n","        self.rank = rank\n","\n","\n","        # Layers.\n","\n","        # Original dense layer.\n","        self.original_layer = original_layer\n","        # No matter whether we are training the model or are in inference mode,\n","        # this layer should be frozen.\n","        None # Set layer as non trainable\n","\n","        # LoRA dense layers.\n","        self.A = None # Set A to be the first Dense layer, don't use bias, how many units should it have? Set the name as lora_A\n","\n","        self.B = None # Set B to be the second Dense layer, don't use bias, how many units should it have? Set the name as lora_B\n","\n","    def call(self, inputs):\n","        original_output = self.original_layer(inputs)\n","        if self.trainable:\n","            # If we are fine-tuning the model, we will add LoRA layers' output\n","            # to the original layer's output.\n","            lora_output = None # Implement lora output\n","            return original_output + lora_output\n","\n","        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n","        # the original layer's weights\n","        return original_output"],"metadata":{"id":"MJoRRBCYntJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will randomly change some Dense layers into Lora Adapted layers"],"metadata":{"id":"Vznpe0o2xOA_"}},{"cell_type":"code","source":["import random\n","# Define a function to replace dense layers with LoraLayer\n","def replace_with_lora(model):\n","    new_model = keras.Sequential()\n","    for layer in model.layers:\n","        if isinstance(layer, Dense) and random.random() > 0.5:\n","            new_model.add(None)  # Add LoraLayer, set accordingly the dim\n","        else:\n","            new_model.add(None) # Else use the layer\n","    return new_model\n","\n","# Replace layers in the model\n","lora_model = replace_with_lora(model)\n","\n","None # Build the model\n","\n","\n","# Compile the model\n","lora_model.compile(None)  # Compile the model\n"],"metadata":{"id":"EgUwvc3-oLcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_model.summary()"],"metadata":{"id":"ssb8zSUxojvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the non-trainable parameters"],"metadata":{"id":"9V3rewF0xVXa"}},{"cell_type":"code","source":["# Fine-tune the model\n","None # Train the model"],"metadata":{"id":"BFHihSHYom3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As mentioned, performance sucks, but the important thing is that we finetuned only the LoraLayers"],"metadata":{"id":"fdv1pMzXxYwW"}},{"cell_type":"code","source":[],"metadata":{"id":"EkJqOtFusXQH"},"execution_count":null,"outputs":[]}]}