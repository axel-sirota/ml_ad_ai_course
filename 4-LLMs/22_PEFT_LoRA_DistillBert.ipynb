{"cells":[{"cell_type":"markdown","source":["# Parameter Efficient Fine Tuning with LoRA"],"metadata":{"id":"bx4HBy0PhvSU"},"id":"bx4HBy0PhvSU"},{"cell_type":"markdown","source":["In this lab we will implement LoRA to finetune DistillBert to perform good Question Anwering with Context"],"metadata":{"id":"pJt-aQkNuE9E"},"id":"pJt-aQkNuE9E"},{"cell_type":"markdown","source":["## Implementing LoRA on DistillBERT\n"],"metadata":{"id":"87fG2m2kt-pQ"},"id":"87fG2m2kt-pQ"},{"cell_type":"markdown","source":["Remember that LoRA is a technique where we use 2 low rank matrices to adapt to the output of a given Layer. Like the following image:\n","\n","\n","<img src='https://www.dropbox.com/scl/fi/dfhuc42h5ohcbfny14gg8/lora.png?rlkey=7ku1ocyzibdgmnkup7kmsd8gb&raw=1'  />\n","\n"],"metadata":{"id":"JXxlI3Xqtx48"},"id":"JXxlI3Xqtx48"},{"cell_type":"code","source":["pip install datasets"],"metadata":{"id":"tG3WiuoFAzbX"},"id":"tG3WiuoFAzbX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TFDistilBertModel, DistilBertConfig, DistilBertTokenizer\n","from transformers.models.distilbert.modeling_tf_distilbert import TFDistilBertMainLayer\n","import tensorflow as tf\n","import keras\n","import math\n","import numpy as np\n","import logging\n","import warnings\n","logging.disable(logging.WARNING)\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"FB04AvOtpwD_"},"id":"FB04AvOtpwD_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First we implement the Lora Layer like we did before"],"metadata":{"id":"RPdpRfEjt9Zo"},"id":"RPdpRfEjt9Zo"},{"cell_type":"code","source":["class LoraLayer(keras.layers.Layer):\n","    def __init__(\n","        self,\n","        original_layer,\n","        rank=8,\n","        num_heads =1,\n","        dim = 1,\n","        trainable=False,\n","        **kwargs,\n","    ):\n","        # We want to keep the name of this layer the same as the original\n","        # dense layer.\n","        original_layer_config = original_layer.get_config()\n","        name = original_layer_config[\"name\"]\n","\n","        kwargs.pop(\"name\", None)\n","\n","        super().__init__(name=name, trainable=trainable, **kwargs)\n","\n","        self.rank = rank\n","\n","\n","        # Layers.\n","\n","        # Original dense layer.\n","        self.original_layer = original_layer\n","        # No matter whether we are training the model or are in inference mode,\n","        # this layer should be frozen.\n","        self.original_layer.trainable = False\n","\n","        # LoRA dense layers.\n","        self.A = keras.layers.Dense(\n","            units=rank,\n","            use_bias=False,\n","            trainable=trainable,\n","            name=f\"lora_A\",\n","        )\n","\n","        self.B = keras.layers.Dense(\n","            units=dim,\n","            use_bias=False,\n","            trainable=trainable,\n","            name=f\"lora_B\",\n","        )\n","\n","    def call(self, inputs):\n","        original_output = self.original_layer(inputs)\n","        if self.trainable:\n","            # If we are fine-tuning the model, we will add LoRA layers' output\n","            # to the original layer's output.\n","            lora_output = self.B(self.A(inputs))\n","            return original_output + lora_output\n","\n","        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n","        # the original layer's weights - more on this in the text generation\n","        # section!\n","        return original_output"],"metadata":{"id":"amS2Bzikp02p"},"id":"amS2Bzikp02p","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we iterate and replace each query and value from the MultiHeadAttention layer to LoraLayers"],"metadata":{"id":"Vsn6K26ZvFWR"},"id":"Vsn6K26ZvFWR"},{"cell_type":"code","source":["# Load DistilBert model\n","config = DistilBertConfig()\n","lora_model = TFDistilBertModel(config)\n","\n","# Iterate through the layers and modify the self-attention layers\n","for layer in lora_model.distilbert.transformer.layer:\n","    attention = None # Grab attention layer\n","    dim = None # The dimension of the model\n","    n_heads = None # The number of heads in the model\n","    # Replace query and value weights with LoraLayer instances\n","    attention.q_lin = None # Replace q_lin with a lora layer\n","    attention.v_lin = None # Replace v_lin with a lora layer"],"metadata":{"id":"TM82NfDoqCG1"},"id":"TM82NfDoqCG1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = DistilBertConfig()\n","standard_model = TFDistilBertModel(config)"],"metadata":{"id":"hj7a27oYqby7"},"id":"hj7a27oYqby7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We test both implementations work"],"metadata":{"id":"PaLdl6qCvQUE"},"id":"PaLdl6qCvQUE"},{"cell_type":"code","source":["test_text = [\"This is a test sentence for DistilBert models.\"]\n","\n","# Load tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Tokenize the input\n","inputs = tokenizer(test_text, return_tensors='tf', padding=True, truncation=True)\n","\n","# Run the input through the standard model\n","standard_output = standard_model(inputs)\n","# Run the input through the LoRA model\n","lora_output = lora_model(inputs)\n","\n"],"metadata":{"id":"DGjT0Jr1qzFb"},"id":"DGjT0Jr1qzFb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we set the non-LoraLayers as non-trainable and calculate the trainable weights per layer"],"metadata":{"id":"b__R_PCSvTX2"},"id":"b__R_PCSvTX2"},{"cell_type":"code","source":["print('Before change: \\n')\n","\n","for layer in lora_model._flatten_layers():\n","    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))\n","\n","    if layer.__class__.__module__.startswith('keras') and not layer.name.startswith(\"lora\"):\n","        layer.trainable = False\n","    elif layer.name.startswith(\"lora\"):\n","        layer.trainable = True\n","    elif layer.name == 'embeddings':\n","        layer.trainable = False\n","\n","print('After change: \\n\\n')\n","\n","for layer in lora_model._flatten_layers():\n","    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))"],"metadata":{"id":"GHdmCjFB14MX"},"id":"GHdmCjFB14MX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def calculate_trainable_params(model):\n","  return None # Implement the amount of trainable weights\n","\n","print(f\"Trainable parameters in standard DistilBert: {calculate_trainable_params(standard_model)}\")\n","print(f\"Trainable parameters in LoRA-adapted DistilBert: {calculate_trainable_params(lora_model)}\")\n"],"metadata":{"id":"LHtaHBXBq3Mr"},"id":"LHtaHBXBq3Mr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the difference!!"],"metadata":{"id":"f_dE8hwXvd93"},"id":"f_dE8hwXvd93"},{"cell_type":"code","source":["standard_model.summary()"],"metadata":{"id":"U0Njbc6fqiC2"},"id":"U0Njbc6fqiC2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_model.summary()"],"metadata":{"id":"A31Kf8gxqNaJ"},"id":"A31Kf8gxqNaJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We succesfully Lora-adapted DistillBERT"],"metadata":{"id":"LHc5tp2ovg5g"},"id":"LHc5tp2ovg5g"},{"cell_type":"markdown","source":["## Q&A"],"metadata":{"id":"aGAy7tBZ5BrV"},"id":"aGAy7tBZ5BrV"},{"cell_type":"markdown","source":["Loading classic stuff to do Q&A plus tokenizing and decoding"],"metadata":{"id":"ixyJj7dDvqP-"},"id":"ixyJj7dDvqP-"},{"cell_type":"code","source":["from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n","from datasets import load_dataset\n","\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","def prepare_qa_input(question, context):\n","    inputs = tokenizer.encode_plus(\n","        question,\n","        context,\n","        add_special_tokens=True,\n","        return_tensors=\"tf\",\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=512  # Adjust based on your needs\n","    )\n","    return inputs"],"metadata":{"id":"wdor8GVi5CW9"},"id":"wdor8GVi5CW9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n"],"metadata":{"id":"1VZsNSyd5C1L"},"id":"1VZsNSyd5C1L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"What is the capital of France?\"\n","context = \"France is a country in Europe. Its capital is Paris.\"\n","\n","inputs = prepare_qa_input(question, context)\n","\n","outputs = model(**inputs)\n","\n","start_logits, end_logits = outputs.start_logits, outputs.end_logits"],"metadata":{"id":"s6GN_iag5FjG"},"id":"s6GN_iag5FjG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def decode_answer(start_logits, end_logits, inputs):\n","    start_index = tf.argmax(start_logits, axis=1)[0]\n","    end_index = tf.argmax(end_logits, axis=1)[0]\n","\n","    # Convert token indices to the original context text\n","    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","    answer_tokens = tokens[start_index: end_index + 1]\n","    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n","\n","    # Check for [CLS] prediction\n","    if answer.startswith(\"[CLS]\"):\n","        return \"Answer not found\"\n","    return answer\n","\n","decoded_answer = decode_answer(start_logits, end_logits, inputs)\n","print(\"Answer:\", decoded_answer)"],"metadata":{"id":"m50zJfsk5LAH"},"id":"m50zJfsk5LAH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lora-Adapt this new model"],"metadata":{"id":"o1RbUssIvwu-"},"id":"o1RbUssIvwu-"},{"cell_type":"code","source":["# Load TFDistilBertForQuestionAnswering model\n","lora_model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n","\n","\n","# Iterate through the layers and modify the self-attention layers\n","for layer in lora_model.distilbert.transformer.layer:\n","    None # Repeat the same as before"],"metadata":{"id":"eHytxq3d5wBg"},"id":"eHytxq3d5wBg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = lora_model(**inputs)\n","\n","start_logits, end_logits = outputs.start_logits, outputs.end_logits\n","decode_answer(start_logits, end_logits, inputs)"],"metadata":{"id":"2UrpUCBy7Vb_"},"id":"2UrpUCBy7Vb_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Before change: \\n')\n","\n","for layer in lora_model._flatten_layers():\n","    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))\n","\n","    if layer.__class__.__module__.startswith('keras') and not layer.name.startswith(\"lora\"):\n","        layer.trainable = False\n","    elif layer.name.startswith(\"lora\"):\n","        layer.trainable = True\n","    elif layer.name == 'embeddings':\n","        layer.trainable = False\n","\n","print('After change: \\n\\n')\n","\n","for layer in lora_model._flatten_layers():\n","    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))"],"metadata":{"id":"WEEIt9bZ7Ntm"},"id":"WEEIt9bZ7Ntm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_model.summary()"],"metadata":{"id":"AIItF-wo7sM1"},"id":"AIItF-wo7sM1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now comes the tricky part, we will  use the squad dataset, that has questions and answers, but we need to add the start_positions and end_positions"],"metadata":{"id":"mm3HeRDlv3PZ"},"id":"mm3HeRDlv3PZ"},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n","\n","# Load the dataset (replace with your specific dataset)\n","dataset = None # Load squad validation split\n","\n","def add_token_positions(encodings, answers):\n","    start_positions = []\n","    end_positions = []\n","    for i in range(len(answers)):\n","        start_char = answers[i]['answer_start'][0]\n","        answer_text = answers[i]['text'][0]  # The text of the answer\n","        end_char = start_char + len(answer_text)  # Calculate the end character position\n","\n","        start_idx = encodings.char_to_token(i, start_char)\n","        end_idx = encodings.char_to_token(i, end_char - 1)  # Adjust by -1 for inclusive range\n","\n","        # Set to 0 (default index) if answer is not found within the context\n","        if start_idx is None:\n","            start_idx = 0\n","        if end_idx is None:\n","            end_idx = start_idx  # Default to start index if end index is not found\n","\n","        start_positions.append(start_idx)\n","        end_positions.append(end_idx)\n","\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","\n","\n","\n","def add_token_positions_batch(examples):\n","    # Tokenize the examples and then add token positions\n","    tokenized_inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding='max_length', return_offsets_mapping=True, max_length=512)\n","    add_token_positions(tokenized_inputs, examples['answers'])\n","    return tokenized_inputs\n","\n","# Apply the function in a batched manner\n","tokenized_datasets = dataset.map(add_token_positions_batch, batched=True, remove_columns=dataset.column_names)"],"metadata":{"id":"ZQ1-8nAYLWhM"},"id":"ZQ1-8nAYLWhM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_ids = tokenized_datasets['input_ids']\n","train_attention_mask = tokenized_datasets['attention_mask']\n","train_start_positions = tokenized_datasets['start_positions']\n","train_end_positions = tokenized_datasets['end_positions']\n"],"metadata":{"id":"KSrqlhOVVf8Z"},"id":"KSrqlhOVVf8Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_ids = None # Convert to tensor\n","train_attention_mask = None # Convert to tensor\n","train_start_positions = None # Convert to tensor\n","train_end_positions = None # Convert to tensor\n"],"metadata":{"id":"Sxl8lTm9XGCb"},"id":"Sxl8lTm9XGCb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing if everything is OK"],"metadata":{"id":"cl7SCsUqwJGl"},"id":"cl7SCsUqwJGl"},{"cell_type":"code","source":["# Example: Checking model weights for NaN values\n","for weight in lora_model.weights:\n","    if tf.reduce_any(tf.math.is_nan(weight)):\n","        print(\"NaN weight:\", weight.name)\n"],"metadata":{"id":"Zv9CFsbOYP--"},"id":"Zv9CFsbOYP--","execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_output = lora_model(\n","    input_ids=train_input_ids[:1],\n","    attention_mask=train_attention_mask[:1]\n",")\n","if tf.reduce_any(tf.math.is_nan(test_output.start_logits)) or tf.reduce_any(tf.math.is_nan(test_output.end_logits)):\n","    print(\"NaN found in model outputs\")\n"],"metadata":{"id":"5BCMxl9hYbRt"},"id":"5BCMxl9hYbRt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finetuning Lora weights to do Q&A on squad"],"metadata":{"id":"GtazWuIFwObI"},"id":"GtazWuIFwObI"},{"cell_type":"code","source":["# Compile the model\n","lora_model.compile(\n","    optimizer= tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")\n"],"metadata":{"id":"o6SLQflz8kMv"},"id":"o6SLQflz8kMv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the callback\n","early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n","# Train the model\n","None # Train for 5 epochs and batch size of 32"],"metadata":{"id":"9xZPGMs3WxVq"},"id":"9xZPGMs3WxVq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_model.summary()"],"metadata":{"id":"FgkM5CNZ1SiY"},"id":"FgkM5CNZ1SiY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your existing code for model prediction\n","question = \"What is the capital of France?\"\n","context = \"France is a country in Europe. Its capital is Paris.\"\n","inputs = prepare_qa_input(question, context)\n","outputs = lora_model(inputs)\n","start_logits, end_logits = outputs.start_logits, outputs.end_logits\n","\n","# Decode the answer\n","decoded_answer = decode_answer(start_logits, end_logits, inputs)\n","print(\"Answer:\", decoded_answer)"],"metadata":{"id":"araKIGJLzrth"},"id":"araKIGJLzrth","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In  case you wanted to create your custom training loop:"],"metadata":{"id":"GC-7WdhjtO5M"},"id":"GC-7WdhjtO5M"},{"cell_type":"code","source":["# import tqdm\n","\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6, epsilon=1e-08, clipnorm=1.0)\n","# loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# # Optionally define metrics, e.g., accuracy\n","# train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","# epochs = 3  # Set the number of epochs\n","\n","# for epoch in range(epochs):\n","#     print(\"\\nStart of epoch %d\" % (epoch,))\n","#     for step, batch in enumerate(tqdm.tqdm(tf_dataset)):\n","\n","#         # Open a GradientTape\n","#         with tf.GradientTape() as tape:\n","\n","#             # Forward pass\n","#             outputs = lora_model(\n","#                 input_ids=batch['input_ids'],\n","#                 attention_mask=batch['attention_mask']\n","#             )\n","\n","\n","#             # Extract start_logits and end_logits from the model's output\n","#             start_logits = outputs.start_logits\n","#             end_logits = outputs.end_logits\n","\n","#             # Compute the loss value\n","#             start_loss = loss_function(batch['start_positions'], start_logits)\n","\n","#             end_loss = loss_function(batch['end_positions'], end_logits)\n","\n","#             # Compute the total loss\n","#             total_loss = (start_loss + end_loss) / 2\n","\n","#         # Compute gradients\n","#         gradients = tape.gradient(total_loss, lora_model.trainable_variables)\n","#         gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n","\n","#         # Update weights\n","#         optimizer.apply_gradients(zip(gradients, lora_model.trainable_variables))\n","\n","#         # Update training metric.\n","#         train_acc_metric.update_state(batch['start_positions'], start_logits)\n","#         train_acc_metric.update_state(batch['end_positions'], end_logits)\n","\n","#         # Log every 200 batches.\n","#         if step % 200 == 0:\n","#             print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(total_loss)))\n","#             print(\"Seen so far: %s samples\" % ((step + 1) * 16))\n","\n","#     # Display metrics at the end of each epoch.\n","#     train_acc = train_acc_metric.result()\n","#     print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","#     # Reset training metrics at the end of each epoch\n","#     train_acc_metric.reset_states()\n"],"metadata":{"id":"obacxZHhajGv"},"id":"obacxZHhajGv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p2NRNMPRdBID"},"id":"p2NRNMPRdBID","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}