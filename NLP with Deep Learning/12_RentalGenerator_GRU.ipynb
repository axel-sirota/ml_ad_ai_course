{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_ad_ai_course/blob/main/NLP%20with%20Deep%20Learning/12_RentalGenerator_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoJdHlUf6c16"
      },
      "source": [
        "# Text Generation with a character based RNN/GRU\n",
        "\n",
        "In this notebook we will train from scratch in an unsupervised fashion text generation to replicate the descriptions of the Airbnb rentals. For time limitations it will not be the best model but it is structured in a portable fashion to your needs.\n",
        "\n",
        "Take it easy and pay attention to the model, how we pass states, and the complexities to do the passthrough of passing the states character by character.\n",
        "\n",
        "You can run this lab both locally or in Colab.\n",
        "\n",
        "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
        "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
        "\n",
        "Follow the instructions. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIKJS_35hR8q"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnbTWB1khR5s"
      },
      "outputs": [],
      "source": [
        "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YkshUXDhR0Y"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, GRU\n",
        "from keras import Model, Input\n",
        "import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import keras_nlp\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "import pickle\n",
        "from tensorflow.nn import leaky_relu\n",
        "\n",
        "import re\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 100\n",
        "rnn_units = 128\n",
        "epochs=50\n",
        "buffer_size = 2000\n",
        "corpus_size=30000\n",
        "test_corpus_size=1000\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "seq_length = 100\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCyoTGiHhRN-"
      },
      "outputs": [],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f train_corpus_descriptions_airbnb.csv ]; then\n",
        "  wget -O train_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/5rp7ibop99qyafo/train_corpus_descriptions_airbnb.csv?dl=0\n",
        "fi\n",
        "\n",
        "if [ ! -f test_corpus_descriptions_airbnb.csv ]; then\n",
        "    wget -O test_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/a29bbkg8hi4q4f4/test_corpus_descriptions_airbnb.csv?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgyETfkigMHP"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1amH2OpgMHQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_path = \"./train_corpus_descriptions_airbnb.csv\"\n",
        "test_path = \"./test_corpus_descriptions_airbnb.csv\"\n",
        "# Read, then decode for py2 compat.\n",
        "airbnb_reviews = pd.read_csv(train_path, header=None, names=[\"review\"]).dropna().sample(n=corpus_size).reset_index(drop=True)\n",
        "test_airbnb_reviews = pd.read_csv(test_path, header=None, names=[\"review\"]).dropna().sample(n=test_corpus_size).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6n1DYQMmnfG"
      },
      "outputs": [],
      "source": [
        "airbnb_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mJb_78qgMHR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Take a look at the first review in text\n",
        "print(airbnb_reviews.iloc[0].review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQlv5nWmnqw0"
      },
      "outputs": [],
      "source": [
        "vocab = set()\n",
        "\n",
        "# Construct the vocabulary, you are free to iterate the DF or use TF utilities. What is key is that the vocab will be of characters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMYUrVwFgMHR"
      },
      "outputs": [],
      "source": [
        "print(f'{len(vocab)} unique characters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9e_y27xgMHS",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Use StringLookup to construct a map with ids_from_chars\n",
        "ids_from_chars = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq5wMDSugMHT"
      },
      "outputs": [],
      "source": [
        "# And construct the inverse\n",
        "chars_from_ids = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czQvLYzDgMHT"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdEueo3ngMHU"
      },
      "outputs": [],
      "source": [
        "ids = ids_from_chars(tf.strings.unicode_split('Only you can prevent forest fires', input_encoding='UTF-8'))\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNacRyjgMHU"
      },
      "outputs": [],
      "source": [
        "text_from_ids(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJBRfRqvvOq3"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, should_join=True):\n",
        "    text = str(text)\n",
        "    text = ' '.join(str(word).lower() for word in textblob_tokenizer(text))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    if should_join:\n",
        "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
        "    else:\n",
        "      return gensim.utils.simple_preprocess(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVm2DhrSt9bs"
      },
      "outputs": [],
      "source": [
        "def get_ids_tensor(df):\n",
        "  # This is a cool trick. Since tf.concat requires same shapes, start with a row os 0's, concat all that's needed and then return all rows but the first\n",
        "  all_ids = tf.constant(np.zeros((1, seq_length)), dtype='int64')\n",
        "\n",
        "  for review in df.review:\n",
        "      review = None # Preprocess the review (should it output text or a list?)\n",
        "      review_length = len(review)\n",
        "      batches = review_length // seq_length\n",
        "      for batch in range(batches):  # As a review may be longer than the seq_length, and the output tensor must have the same width, we need to batch\n",
        "        lower_limit = None  # Fill lower limit of batch in the review\n",
        "        upper_limit = None  # Fill upper limit of batch in the review (we will drop the final remainder)\n",
        "        value = None # Get tensor of ids for each character position\n",
        "        value = tf.reshape(value, [1, 100])  # In case the end shape is different\n",
        "        output = tf.concat([all_ids,value], axis=0)  # Add row of that batch\n",
        "        all_ids = tf.reshape(output, [-1, 100])  # Ensure column width\n",
        "  return all_ids[1:]\n",
        "\n",
        "all_ids = get_ids_tensor(df=airbnb_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "O1SKexbC6c2G"
      },
      "outputs": [],
      "source": [
        "print(all_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNKd5EcE-b-y"
      },
      "outputs": [],
      "source": [
        "ids_from_chars(tf.strings.unicode_split(airbnb_reviews.review[0][0:100], 'UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTyicxUO4ER7"
      },
      "outputs": [],
      "source": [
        "test_all_ids = get_ids_tensor(df=test_airbnb_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo1Wgt5agMHV"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "ids_dataset = None # Create tensorflow Dataset object from all_ids tensor\n",
        "test_ids_dataset = None # Do likewise for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6s18du1gMHW"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYp6t2KgMHW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dataset = ids_dataset.map(split_input_target)\n",
        "test_dataset = test_ids_dataset.map(split_input_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzym60TTgMHW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYRoOShmgMHX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size=batch_size, drop_remainder=True)\n",
        ")\n",
        "test_dataset = (\n",
        "    test_dataset\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size=batch_size, drop_remainder=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GR56NhN2-iL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp6f_sr-gMHX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class RentalGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units, seq_length):\n",
        "    super().__init__(self)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_length = seq_length\n",
        "    self.embedding = None # Create Embedding layer, which should be its shape? If you want test out training a word2vec and set it here to fine-tune\n",
        "    self.rnn = None # Create a GRU layer with tanh activation to ensure you can use cuDNN libraries. You should return both sequences and states\n",
        "    self.dense = None # Final Dense layer as always, should you return a softmax?\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = None # Apply embedding, pass the training var\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = None, None # Apply rnn layer, passing the states\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(self.seq_length, ))   # doesn't consider the batch\n",
        "    Model(inputs=x, outputs=self.call(x))\n",
        "    self.build((None, self.seq_length, ))   # takes into consideration the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ0cvE_YgMHY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "model = RentalGenerator(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    seq_length=seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dcNCEWogMHY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "model.build_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ki372A4p6c2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "z4GAjp826c2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzX4F9z-gMHY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Set a perplexity metric\n",
        "perplexity = None\n",
        "# Compile the model, passing perplexity as the metric and check that the loss is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYw8nf2agMHZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.05)\n",
        "history = None # train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KQWnLby8G5t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function for plotting loss\n",
        "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(train_metric,color='blue',label=metric_name)\n",
        "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "# plot loss history\n",
        "plot_metrics(history.history['loss'], val_metric=history.history['val_loss'], metric_name=\"Loss\", title=\"Loss\", ylim=5.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WQEZfLg8Hxd"
      },
      "outputs": [],
      "source": [
        "plot_metrics(history.history['perplexity'], val_metric=history.history['val_perplexity'], metric_name=\"perplexity\", title=\"perplexity\", ylim=20.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy0-768EgMHa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv2UPlTWgMHa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Midtown Sunny 2-Bedroom'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(250):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVWZTnQegMHa"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(one_step_model, 'rental_generator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDj6agGJCZdh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}