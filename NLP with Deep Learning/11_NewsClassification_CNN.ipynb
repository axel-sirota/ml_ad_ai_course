{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_ad_ai_course/blob/main/NLP%20with%20Deep%20Learning/11_NewsClassification_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEqSAWGgNeRx"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IY2pTPBPir1"
      },
      "outputs": [],
      "source": [
        "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gZklVraP2vn"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
        "import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "import pickle\n",
        "from tensorflow.nn import leaky_relu\n",
        "\n",
        "import re\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 300\n",
        "n_channels = 64\n",
        "p_dropout = 0.2\n",
        "epochs=1000\n",
        "batch_size = 500\n",
        "corpus_size=100000\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0DZp9-YP8Fa"
      },
      "outputs": [],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f news.csv ]; then\n",
        "  wget -O news.csv https://www.dropbox.com/s/352x7xzivf60zgc/news.csv?dl=0\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arc-2X3AS2dI"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIsjpoQMS5tT"
      },
      "outputs": [],
      "source": [
        "path = './news.csv'\n",
        "news_pre = pd.read_csv(path, header=0).sample(n=corpus_size).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e48igNipS-7m"
      },
      "outputs": [],
      "source": [
        "# We will reuse what we learned in previous labs!\n",
        "def preprocess_text(text, should_join=True):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    if should_join:\n",
        "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
        "    else:\n",
        "      return gensim.utils.simple_preprocess(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmuzxBe0S-45"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing and write to a csv file. Ensure it is the same the MyCorpus object is reading\n",
        "news = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI4j9GohS-1k"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = 'news_processed.csv'\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield preprocess_text(line, should_join=False)\n",
        "\n",
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "word2vec = gensim.models.Word2Vec(sentences=sentences, vector_size=embedding_dim, epochs=10)\n",
        "word2vec_model = word2vec.wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g02Wjl0NS-y2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the embedding weights and vocab\n",
        "weights = None\n",
        "vocab_size = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS0pq9sbO7hy"
      },
      "outputs": [],
      "source": [
        "weights.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "news_preprocessed = pd.DataFrame()\n",
        "news_preprocessed['label'] = news_pre.category.map({'Business': 0, 'Sports': 1, 'Sci/Tech': 2, 'World': 3})\n",
        "news_preprocessed['title'] = news\n",
        "news_preprocessed"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5MpWk95pjGp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fsl1VwnVJoz"
      },
      "outputs": [],
      "source": [
        "def get_maximum_review_length(df):\n",
        "    maximum = 0\n",
        "    for ix, row in df.iterrows():\n",
        "        candidate = len(tokenizer(row.title))\n",
        "        if candidate > maximum:\n",
        "            maximum = candidate\n",
        "    return maximum\n",
        "\n",
        "\n",
        "maximum = get_maximum_review_length(news_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cFi0e_sVJmh"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(news_preprocessed), maximum))   # Here we do what we said above\n",
        "# Iterate through the news df and for every word, if it exists in the word2vec model, put into X for that review and that word the index of the embedding (check index_to_key)\n",
        "# HINT: to iterate through a column of a pandas dataframe you do:\n",
        "\n",
        "# for index, value in df.iterrows():\n",
        "#      #do something\n",
        "#\n",
        "# FILL\n",
        "y = news_preprocessed.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HspIJVUBVJkF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = tf.constant(X_train)\n",
        "X_test = tf.constant(X_test)\n",
        "y_train = tf.one_hot(tf.constant(y_train), 4)\n",
        "y_test = tf.one_hot(tf.constant(y_test), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z5V4riHX8lf"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSvBEG4_aV-3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWpiksEJYWyr"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzrIQ4G0VJhh"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=weights.shape[0], output_dim=embedding_dim, input_length=maximum, embeddings_initializer=Constant(weights), trainable=True))\n",
        "model.add()  # Add a Conv1d layer tp have n_channels filters of 3x3. It is key you ensure you don't loose dimension size!\n",
        "model.add()  # Add an activation layer\n",
        "model.add() # Add another Conv1d layer tp have 2*n_channels filters of 3x3. It is key you ensure you don't loose dimension size!\n",
        "model.add()  # Add an activation layer\n",
        "model.add()  # Average out the convolution dimension\n",
        "model.add()  # You can add several Dense and batch norm layers if you prefer\n",
        "model.add()  # Add a dropout\n",
        "model.add()  # Final Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T-hPDiIVJe9"
      },
      "outputs": [],
      "source": [
        "# Compile the model. Think what is the best loss to use\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCQiCKLSVJcg"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=0.01)\n",
        "history = None # Fit the model, use the callback above to do EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDKdanUVa4GT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function for plotting loss\n",
        "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(train_metric,color='blue',label=metric_name)\n",
        "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "# plot loss history\n",
        "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSmRIcRxbzin"
      },
      "outputs": [],
      "source": [
        "# Plot whatever metric you defined in the compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1fTU2I2V5__"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G46G9Aubedz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}