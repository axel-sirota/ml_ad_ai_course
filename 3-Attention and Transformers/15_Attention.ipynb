{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_ad_ai_course/blob/main/Attention%20and%20Transformers/15_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edRPa0-vYVcA"
   },
   "source": [
    "# Introducing Attention\n",
    "\n",
    "© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "Attention is one of the most groundbreaking ideas that revolutionized NLP and AI on the latest years. However, it is difficult to encounter a demo that is solely focused on attention... until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-TMj7ciY1R9"
   },
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTDmOLKh4gJy",
    "outputId": "17b79132-72dd-43e0-b33c-7d44cfaf8fb2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"word2vec_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw0pwirWXuYA"
   },
   "source": [
    "Let's define some helper functions we need:\n",
    "\n",
    "* The softmax funciton definition for Numpy arrays\n",
    "* An Embedder that transforms a list of words into its embedding representation according to `word2vec_sample` from the package `nltk`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cc9Eevlj573a"
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd99pIV059c5"
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(words):\n",
    "    \"\"\"\n",
    "    Function that takes in a list of words and returns a list of their embeddings,\n",
    "    based on a pretrained word2vec encoder.\n",
    "    \"\"\"\n",
    "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        word2vec_sample, binary=False\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    words_pass = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(np.array(model.word_vec(word)))\n",
    "            words_pass.append(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    embeddings = np.array(output)\n",
    "    del model  # free up space again\n",
    "    return embeddings, words_pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbBCFpJRNSeq"
   },
   "source": [
    "## Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzHIp_SzdYi6"
   },
   "source": [
    "The idea behind attention is simple, if you take any word, like `Apple`, its meaning will change with respect with the other words in the sentence. For example below, In the first sentence Apple refers to the company and has strong relationship with coding and computer; on the second one refers to the fruit and therefore at most it would have relationship with eating, but not coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVj0D19CcHCk"
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src='https://www.dropbox.com/s/91xzqre8dpvxrux/sentence.png?raw=1' alt=\"drawing\" width=\"350\" />\n",
    "<figcaption>Words relevance change with context</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMJ6SbhaeeO5"
   },
   "source": [
    "  What I just spoke, is known as **Cross Attention**, because you will calculate the relationship of one word with respect to **all** the others in the sentence. In an image it would be:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://www.dropbox.com/s/ahn8ogriuzasa9a/attention_in_detail.png?raw=1'  />\n",
    "<figcaption>Attention</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu-KMY65gLvJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcYDNPZsfqtv"
   },
   "source": [
    "In code it is even easier, don't worry about the image above it will make sense as we evolve through the course. The really important part is the following:\n",
    "\n",
    "$$\n",
    "a_{ij} = f(h_i, s_{j})\n",
    "$$\n",
    "\n",
    "Where $a_{i,j}$ stands for the alignment of the word `h_i` with the output word `s_j`. The alignment may sound fancy, but it simply means how strongly connected those 2 words are in that sentence, like the Apple example!\n",
    "\n",
    "\n",
    "The key is that the function $f$ can be anything. In the original paper, and the one we are implementing now it is the dot product, which you have probably seen before, and if not check the course I referenced before,  **Implement Natural Language Processing for Word Embedding**:\n",
    "\n",
    "$$\n",
    "a_{i,j} = dot product(h_i, s_j) = h_i^T*s_j\n",
    "$$\n",
    "\n",
    "So this means that for a given initial word, which is a row in the matrix we created, we have a Tensor of how aligned it is with that output word; we call that Tensor `c_k` or context vector.\n",
    "\n",
    "And here comes the important stuff number 2, which is we take softmax to obtain weights, those wieghts will tell me for that input word how much weight (and importance) I should put into any output word. That is the attention matrix.\n",
    "\n",
    "$$\n",
    "z_j = softmax_k(c_{j,k})\n",
    "$$\n",
    "\n",
    "If we multiply this with the context vector of an encoder we have an empowered context memory tensor that can be fed into the decoder, as it is done in Transformers. We will implement all of this alongside this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZGYeCxV5_nA"
   },
   "outputs": [],
   "source": [
    "def dot_product_attention(hidden_states, previous_state):\n",
    "\n",
    "    # Fill here the final attention weights\n",
    "    # [T,d]*[d,N] -> [T,N]\n",
    "    scores = None\n",
    "    w_n = softmax(scores)\n",
    "\n",
    "    # Fill here the weighted average of the weights and hidden states\n",
    "    # [T,N]*[N,d] -> [T,d]\n",
    "    c_t = None\n",
    "\n",
    "    return w_n, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETA3QPEvf__v"
   },
   "source": [
    "Now we will use a helper function that will plot those attention weights I told you about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTZbCxr26Bgb"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
    "    plt.title(\"Attention matrix\")\n",
    "    plt.xlabel(\"Attention score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnUjZSH1l_BQ"
   },
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4bXyz_PjNmV"
   },
   "source": [
    "Let's try with some words related to royalty and some related to food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "uq2d3BZj6C5j",
    "outputId": "c27c1cea-86e9-4254-c5c1-186f2291ebc7"
   },
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
    "word_embeddings, words = get_word2vec_embedding(words)\n",
    "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
    "plot_attention_weight_matrix(weights, words, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsd6Ynvfj5N9"
   },
   "source": [
    "As you can see, this was successful! We could detect the relationships between apple, pear and a little less food; aas one cluster. Then another cluster of the royalty, and finally commputers alone, so it detected what it is supposed to! In the next demo we will implement other forms of attention, ie: changing that function `f`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_rq2dmdsyRA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlMiC8Oof1f9"
   },
   "source": [
    "## Introducing self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOwkqwaFf5xC"
   },
   "source": [
    "In order to get closer to the Transformer we will neeed to understand Se;f Attention. this concept simply relates attention with a database. In a database you have a key value pair, and with a query you get a key and with that key you return the value, right?\n",
    "\n",
    "In self-attention ( or sometimes called  Q, K, V attention) we do the same, but instead of getting one key we will get:\n",
    "\n",
    "$$a_{i, k} = similarity(Q_i, K_k)$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ c_i = ∑_{k}a_{i,k}*v_k $$\n",
    "\n",
    "Which basically means we get a weighted average of **ALL** the values for every input word we want to calculate the alignment. Therefore the term self attention. The diference with the previous attention is that before the similarity function was the dot product, and the matrices `K,Q,V` where the identity (only ones in the diagonal) and here they are learneable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPR2YRs5hyp7"
   },
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04MzMgwksyfH"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
    "    plt.title(\"Attention matrix\")\n",
    "    plt.xlabel(\"Attention score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJzGDX_d6EZb"
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    # Embed a sentence using word2vec; for example use cases only.\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    words = sentence.split()\n",
    "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
    "    return np.expand_dims(word_vector_sequence, axis=0), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o-r0urHh1g7"
   },
   "source": [
    "### Seeing the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W-OoTWZiLhq"
   },
   "source": [
    "In order to see self attention, we just need to do what we did before! But now we will use a variant called **Scaled self attention** which is the one Transformers almost use:\n",
    "\n",
    "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQphfJ45i7-n"
   },
   "source": [
    "It is *very* important to remark all of this later will be done  by the Tensorflow or PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w70po97I8lC0"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    pass\n",
    "    return value, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL8GwjiPjEnB"
   },
   "source": [
    "### Testing it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-kBX6ws8lM8",
    "outputId": "7969f381-d5b5-41bf-b571-6f0174ac4740"
   },
   "outputs": [],
   "source": [
    "sentence = \"I drink coke, but eat steak\"\n",
    "word_embeddings, words = embed_sentence(sentence)\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "pja4Rpzv8m1A",
    "outputId": "f12b6bc5-5c75-41e1-9684-fdec751b84f2"
   },
   "outputs": [],
   "source": [
    "Q = K = V = word_embeddings   # Para no entrenar\n",
    "\n",
    "# calculate weights and plot\n",
    "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "words = re.sub(r\"[^\\w\\s]\", \"\", sentence).split()\n",
    "plot_attention_weight_matrix(attention_weights[0], words, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_FoLZnejI1z"
   },
   "source": [
    "In effect you can see a positive alignment between drink and coke, as well as eat and steak."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMGE2lZraFP/eHRd0P7IgE",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}